{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQFCLASOTdDp"
   },
   "source": [
    "# Training Hugging Face Transformer model with Custom NER Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Eq6T2HuTa4h"
   },
   "source": [
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WSJrwT_c-e9P",
    "outputId": "66a8a65e-36f5-440f-d23d-86ba5d0c7c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 5.2 MB/s \n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
      "\u001b[K     |████████████████████████████████| 290 kB 40.8 MB/s \n",
      "\u001b[?25hCollecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 1.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Collecting huggingface-hub>=0.0.17\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 40.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 36.6 MB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 35.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 43.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 33.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 44.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 46.7 MB/s \n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 45.5 MB/s \n",
      "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=5f8f95fb4f7257de576192b4739837a62109718a4160103cad935af84137f05c\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "Successfully built seqeval\n",
      "Installing collected packages: multidict, yarl, async-timeout, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, seqeval, datasets\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.14.0 fsspec-2021.10.1 huggingface-hub-0.0.19 multidict-5.2.0 pyyaml-6.0 sacremoses-0.0.46 seqeval-1.2.2 tokenizers-0.10.3 transformers-4.11.3 xxhash-2.0.2 yarl-1.7.0\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipQQlaf1-0Lx",
    "outputId": "ea5904e4-0927-461b-8cb5-1d5b47b1315c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mozhi-datasets'...\n",
      "remote: Enumerating objects: 44, done.\u001b[K\n",
      "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
      "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
      "remote: Total 44 (delta 20), reused 31 (delta 11), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (44/44), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/gyan42/mozhi-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFnihknZSS_Z",
    "outputId": "f1ddc81b-aa3b-4d79-8922-2794e23dd9fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/mozhi-datasets/sroie2019\n"
     ]
    }
   ],
   "source": [
    "% cd mozhi-datasets/sroie2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKE0HVk9TuFf",
    "outputId": "6f199d1d-02b8-4325-a11c-27e591e81392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_model_train.py  README.md\t\t SroieNERDataset.ipynb\n",
      "hf_tokenize.py\t   sroie2019_dataset.py  version1\n"
     ]
    }
   ],
   "source": [
    "! ls \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gGGB8Eujlpqe",
    "outputId": "136e30cf-2361-4405-ba23-b8b28616a8b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 2814837..f54c614\n",
      "Fast-forward\n",
      " sroie2019/hf_model_train.py |  9 \u001b[32m++++++\u001b[m\u001b[31m---\u001b[m\n",
      " sroie2019/hf_tokenize.py    | 24 \u001b[32m++++++++++++++++++++++++\u001b[m\n",
      " 2 files changed, 30 insertions(+), 3 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "! git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPjtyBx1kQ8D",
    "outputId": "d42981f8-c531-4084-c574-8bcd9e4de664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory:  /root/.mozhi\n",
      "Cache1 directory:  /root/.mozhi/sroie2019/SROIE2019/1.0.0\n",
      "Reusing dataset sroie2019 (/root/.mozhi/sroie2019/SROIE2019/1.0.0)\n",
      "100% 3/3 [00:00<00:00, 551.52it/s]\n",
      "100% 1/1 [00:00<00:00,  1.31ba/s]\n",
      "100% 1/1 [00:00<00:00, 18.78ba/s]\n",
      "100% 1/1 [00:00<00:00, 13.72ba/s]\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 588\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 49\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 82\n",
      "    })\n",
      "})\n",
      "****************************************************************************************************\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'id', 'input_ids', 'labels', 'ner_tags', 'tokens'],\n",
      "        num_rows: 588\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['attention_mask', 'id', 'input_ids', 'labels', 'ner_tags', 'tokens'],\n",
      "        num_rows: 49\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['attention_mask', 'id', 'input_ids', 'labels', 'ner_tags', 'tokens'],\n",
      "        num_rows: 82\n",
      "    })\n",
      "})\n",
      "First sample:  {'ner_tags': [0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': '0', 'tokens': ['TAN', 'WOON', 'YANN', 'BOOK', 'TA', '.K(TAMAN', 'DAYA)', 'SDN', 'BND', '789417-W', 'NO.53', '55', '57', '&', '59', '', 'JALAN', 'SAGU', '18', 'TAMAN', 'DAYA', '81100', 'JOHOR', 'BAHRU', 'JOHOR.', 'DOCUMENT', 'NO', ':', 'TD01167104', 'DATE:', '25/12/2018', '8:13:39', 'PM', 'CASHIER:', 'MANIS', 'MEMBER:', 'CASH', 'BILL', 'CODE/DESC', 'PRICE', 'DISC', 'AMOUNT', 'QTY', 'RM', '9556939040116', 'KF', 'MODELLING', 'CLAY', 'KIDDY', 'FISH', '1', 'PC', '*', '9.000', '0.00', '9.00', 'TOTAL:', 'ROUR', 'DING', 'ADJUSTMENT:', 'ROUND', 'D', 'TOTAL', '(RM):', 'CASH', '10.00', 'CHANGE', '1.00', 'GOODS', 'SOLD', 'ARE', 'NOT', 'RETURNABLE', 'OR', 'EXCHANGEABLE', '***', 'THANK', 'YOU', 'PLEASE', 'COME', 'AGAIN', '!']}\n",
      "****************************************************************************************************\n",
      "First tokenized sample:  {'input_ids': [101, 9092, 15854, 2078, 13619, 2078, 2338, 11937, 1012, 1047, 1006, 17214, 2319, 2154, 2050, 1007, 17371, 2078, 24869, 2094, 6275, 2683, 23632, 2581, 1011, 1059, 2053, 1012, 5187, 4583, 5401, 1004, 5354, 28410, 7842, 12193, 2324, 17214, 2319, 2154, 2050, 6282, 18613, 25268, 8670, 8093, 2226, 25268, 1012, 6254, 2053, 1024, 14595, 24096, 16048, 2581, 10790, 2549, 3058, 1024, 2423, 1013, 2260, 1013, 2760, 1022, 1024, 2410, 1024, 4464, 7610, 5356, 3771, 1024, 23624, 2015, 2266, 1024, 5356, 3021, 3642, 1013, 4078, 2278, 3976, 5860, 3815, 1053, 3723, 28549, 5345, 26976, 2683, 23499, 2692, 12740, 14526, 2575, 1047, 2546, 19518, 5726, 25358, 2100, 3869, 1015, 7473, 1008, 1023, 1012, 2199, 1014, 1012, 4002, 1023, 1012, 4002, 2561, 1024, 20996, 3126, 22033, 19037, 1024, 2461, 1040, 2561, 1006, 28549, 1007, 1024, 5356, 2184, 1012, 4002, 2689, 1015, 1012, 4002, 5350, 2853, 2024, 2025, 2709, 3085, 2030, 3863, 3085, 1008, 1008, 1008, 4067, 2017, 3531, 2272, 2153, 999, 102], 'labels': [-100, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], 'tokens': ['TAN', 'WOON', 'YANN', 'BOOK', 'TA', '.K(TAMAN', 'DAYA)', 'SDN', 'BND', '789417-W', 'NO.53', '55', '57', '&', '59', '', 'JALAN', 'SAGU', '18', 'TAMAN', 'DAYA', '81100', 'JOHOR', 'BAHRU', 'JOHOR.', 'DOCUMENT', 'NO', ':', 'TD01167104', 'DATE:', '25/12/2018', '8:13:39', 'PM', 'CASHIER:', 'MANIS', 'MEMBER:', 'CASH', 'BILL', 'CODE/DESC', 'PRICE', 'DISC', 'AMOUNT', 'QTY', 'RM', '9556939040116', 'KF', 'MODELLING', 'CLAY', 'KIDDY', 'FISH', '1', 'PC', '*', '9.000', '0.00', '9.00', 'TOTAL:', 'ROUR', 'DING', 'ADJUSTMENT:', 'ROUND', 'D', 'TOTAL', '(RM):', 'CASH', '10.00', 'CHANGE', '1.00', 'GOODS', 'SOLD', 'ARE', 'NOT', 'RETURNABLE', 'OR', 'EXCHANGEABLE', '***', 'THANK', 'YOU', 'PLEASE', 'COME', 'AGAIN', '!'], 'ner_tags': [0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': '0', 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "!python hf_tokenize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YF5LWAHMqc9x",
    "outputId": "5d03e094-be71-4764-9e51-b727b5fd29d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory:  /root/.mozhi\n",
      "Cache1 directory:  /root/.mozhi/sroie2019/SROIE2019/1.0.0\n",
      "Downloading and preparing dataset sroie2019/SROIE2019 to /root/.mozhi/sroie2019/SROIE2019/1.0.0...\n",
      "  0% 0/3 [00:00<?, ?it/s]\n",
      "Downloading: 590kB [00:00, 28.7MB/s]       \n",
      " 33% 1/3 [00:00<00:00,  3.82it/s]\n",
      "Downloading: 50.1kB [00:00, 23.3MB/s]       \n",
      " 67% 2/3 [00:00<00:00,  4.53it/s]\n",
      "Downloading: 68.8kB [00:00, 29.5MB/s]       \n",
      "100% 3/3 [00:00<00:00,  4.59it/s]\n",
      "100% 3/3 [00:00<00:00, 1216.80it/s]\n",
      "Dataset sroie2019 downloaded and prepared to /root/.mozhi/sroie2019/SROIE2019/1.0.0. Subsequent calls will reuse this data.\n",
      "100% 3/3 [00:00<00:00, 425.21it/s]\n",
      "Dataset({\n",
      "    features: ['id', 'tokens', 'ner_tags'],\n",
      "    num_rows: 588\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'tokens', 'ner_tags'],\n",
      "    num_rows: 82\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'tokens', 'ner_tags'],\n",
      "    num_rows: 49\n",
      "})\n",
      "List of tags:  ['O', 'address', 'company', 'date', 'total']\n",
      "First sample:  {'id': '0', 'ner_tags': [0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['TAN', 'WOON', 'YANN', 'BOOK', 'TA', '.K(TAMAN', 'DAYA)', 'SDN', 'BND', '789417-W', 'NO.53', '55', '57', '&', '59', '', 'JALAN', 'SAGU', '18', 'TAMAN', 'DAYA', '81100', 'JOHOR', 'BAHRU', 'JOHOR.', 'DOCUMENT', 'NO', ':', 'TD01167104', 'DATE:', '25/12/2018', '8:13:39', 'PM', 'CASHIER:', 'MANIS', 'MEMBER:', 'CASH', 'BILL', 'CODE/DESC', 'PRICE', 'DISC', 'AMOUNT', 'QTY', 'RM', '9556939040116', 'KF', 'MODELLING', 'CLAY', 'KIDDY', 'FISH', '1', 'PC', '*', '9.000', '0.00', '9.00', 'TOTAL:', 'ROUR', 'DING', 'ADJUSTMENT:', 'ROUND', 'D', 'TOTAL', '(RM):', 'CASH', '10.00', 'CHANGE', '1.00', 'GOODS', 'SOLD', 'ARE', 'NOT', 'RETURNABLE', 'OR', 'EXCHANGEABLE', '***', 'THANK', 'YOU', 'PLEASE', 'COME', 'AGAIN', '!']}\n"
     ]
    }
   ],
   "source": [
    "! python sroie2019_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuXCB9eYqlLn",
    "outputId": "615812e4-29aa-4314-c16d-4bb4917997ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/root/.cache/huggingface/datasets/sroie2019/SROIE2019/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! ls /root/.cache/huggingface/datasets/sroie2019/SROIE2019/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "foMBuqFBTwkT",
    "outputId": "986e7f14-f93f-4a82-b9cb-fe5b0ce6cd0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory:  /root/.mozhi\n",
      "Cache1 directory:  /root/.mozhi/sroie2019/SROIE2019/1.0.0\n",
      "Reusing dataset sroie2019 (/root/.mozhi/sroie2019/SROIE2019/1.0.0)\n",
      "100% 3/3 [00:00<00:00, 541.22it/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at /root/.mozhi/sroie2019/SROIE2019/1.0.0/cache-4d99ee0792dba627.arrow\n",
      "Loading cached processed dataset at /root/.mozhi/sroie2019/SROIE2019/1.0.0/cache-67c9f0fd2fd1c19d.arrow\n",
      "100% 1/1 [00:00<00:00, 13.23ba/s]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'id', 'input_ids', 'labels', 'ner_tags', 'tokens'],\n",
      "    num_rows: 588\n",
      "})\n",
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running training *****\n",
      "  Num examples = 588\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1850\n",
      "  2% 37/1850 [00:38<29:13,  1.03it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.16it/s]\u001b[A\n",
      " 75% 3/4 [00:00<00:00,  3.74it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: company seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: address seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: total seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: date seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.28654298186302185, 'eval_precision': 0.19101123595505617, 'eval_recall': 0.1619047619047619, 'eval_f1': 0.17525773195876287, 'eval_accuracy': 0.9308131241084165, 'eval_runtime': 1.3484, 'eval_samples_per_second': 36.339, 'eval_steps_per_second': 2.966, 'epoch': 1.0}\n",
      "  2% 37/1850 [00:39<29:13,  1.03it/s]\n",
      "100% 4/4 [00:01<00:00,  3.74it/s]\u001b[A\n",
      "  4% 74/1850 [01:18<29:53,  1.01s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.08it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.20435258746147156, 'eval_precision': 0.4267241379310345, 'eval_recall': 0.4714285714285714, 'eval_f1': 0.44796380090497734, 'eval_accuracy': 0.9436519258202568, 'eval_runtime': 1.3639, 'eval_samples_per_second': 35.926, 'eval_steps_per_second': 2.933, 'epoch': 2.0}\n",
      "  4% 74/1850 [01:20<29:53,  1.01s/it]\n",
      "100% 4/4 [00:01<00:00,  3.67it/s]\u001b[A\n",
      "  6% 111/1850 [02:01<33:12,  1.15s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.01it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.1484440267086029, 'eval_precision': 0.5404040404040404, 'eval_recall': 0.5095238095238095, 'eval_f1': 0.5245098039215687, 'eval_accuracy': 0.9526569186875892, 'eval_runtime': 1.397, 'eval_samples_per_second': 35.075, 'eval_steps_per_second': 2.863, 'epoch': 3.0}\n",
      "  6% 111/1850 [02:02<33:12,  1.15s/it]\n",
      "100% 4/4 [00:01<00:00,  3.60it/s]\u001b[A\n",
      "  8% 148/1850 [02:42<26:04,  1.09it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.05it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.1336250901222229, 'eval_precision': 0.4827586206896552, 'eval_recall': 0.5333333333333333, 'eval_f1': 0.5067873303167421, 'eval_accuracy': 0.9575606276747504, 'eval_runtime': 1.3937, 'eval_samples_per_second': 35.157, 'eval_steps_per_second': 2.87, 'epoch': 4.0}\n",
      "  8% 148/1850 [02:43<26:04,  1.09it/s]\n",
      "100% 4/4 [00:01<00:00,  3.64it/s]\u001b[A\n",
      " 10% 185/1850 [03:24<31:07,  1.12s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.00it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.11750789731740952, 'eval_precision': 0.5652173913043478, 'eval_recall': 0.5571428571428572, 'eval_f1': 0.5611510791366906, 'eval_accuracy': 0.9626426533523538, 'eval_runtime': 1.393, 'eval_samples_per_second': 35.176, 'eval_steps_per_second': 2.871, 'epoch': 5.0}\n",
      " 10% 185/1850 [03:25<31:07,  1.12s/it]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      " 12% 222/1850 [04:05<26:57,  1.01it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.03it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.11279411613941193, 'eval_precision': 0.5714285714285714, 'eval_recall': 0.6666666666666666, 'eval_f1': 0.6153846153846153, 'eval_accuracy': 0.9662981455064193, 'eval_runtime': 1.3944, 'eval_samples_per_second': 35.14, 'eval_steps_per_second': 2.869, 'epoch': 6.0}\n",
      " 12% 222/1850 [04:07<26:57,  1.01it/s]\n",
      "100% 4/4 [00:01<00:00,  3.62it/s]\u001b[A\n",
      " 14% 259/1850 [04:47<26:36,  1.00s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.98it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.1170056089758873, 'eval_precision': 0.6228070175438597, 'eval_recall': 0.6761904761904762, 'eval_f1': 0.6484018264840182, 'eval_accuracy': 0.9684379457917262, 'eval_runtime': 1.3996, 'eval_samples_per_second': 35.011, 'eval_steps_per_second': 2.858, 'epoch': 7.0}\n",
      " 14% 259/1850 [04:48<26:36,  1.00s/it]\n",
      "100% 4/4 [00:01<00:00,  3.56it/s]\u001b[A\n",
      " 16% 296/1850 [05:28<22:58,  1.13it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.04it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.1075950637459755, 'eval_precision': 0.6578947368421053, 'eval_recall': 0.7142857142857143, 'eval_f1': 0.684931506849315, 'eval_accuracy': 0.9734308131241084, 'eval_runtime': 1.3887, 'eval_samples_per_second': 35.284, 'eval_steps_per_second': 2.88, 'epoch': 8.0}\n",
      " 16% 296/1850 [05:29<22:58,  1.13it/s]\n",
      "100% 4/4 [00:01<00:00,  3.61it/s]\u001b[A\n",
      " 18% 333/1850 [06:09<26:44,  1.06s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.03it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.10278626531362534, 'eval_precision': 0.658008658008658, 'eval_recall': 0.7238095238095238, 'eval_f1': 0.6893424036281179, 'eval_accuracy': 0.9744115549215406, 'eval_runtime': 1.4013, 'eval_samples_per_second': 34.969, 'eval_steps_per_second': 2.855, 'epoch': 9.0}\n",
      " 18% 333/1850 [06:10<26:44,  1.06s/it]\n",
      "100% 4/4 [00:01<00:00,  3.60it/s]\u001b[A\n",
      " 20% 370/1850 [06:51<23:46,  1.04it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.98it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.11799189448356628, 'eval_precision': 0.5612648221343873, 'eval_recall': 0.6761904761904762, 'eval_f1': 0.6133909287257019, 'eval_accuracy': 0.9679029957203994, 'eval_runtime': 1.4032, 'eval_samples_per_second': 34.92, 'eval_steps_per_second': 2.851, 'epoch': 10.0}\n",
      " 20% 370/1850 [06:52<23:46,  1.04it/s]\n",
      "100% 4/4 [00:01<00:00,  3.57it/s]\u001b[A\n",
      " 22% 407/1850 [07:32<26:57,  1.12s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.02it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.11860522627830505, 'eval_precision': 0.6058091286307054, 'eval_recall': 0.6952380952380952, 'eval_f1': 0.647450110864745, 'eval_accuracy': 0.9698644793152639, 'eval_runtime': 1.3973, 'eval_samples_per_second': 35.068, 'eval_steps_per_second': 2.863, 'epoch': 11.0}\n",
      " 22% 407/1850 [07:34<26:57,  1.12s/it]\n",
      "100% 4/4 [00:01<00:00,  3.57it/s]\u001b[A\n",
      " 24% 444/1850 [08:14<26:49,  1.15s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.02it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.11944987624883652, 'eval_precision': 0.5882352941176471, 'eval_recall': 0.7142857142857143, 'eval_f1': 0.6451612903225806, 'eval_accuracy': 0.9685271041369472, 'eval_runtime': 1.3907, 'eval_samples_per_second': 35.234, 'eval_steps_per_second': 2.876, 'epoch': 12.0}\n",
      " 24% 444/1850 [08:15<26:49,  1.15s/it]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      " 26% 481/1850 [08:56<23:28,  1.03s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.03it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.11202674359083176, 'eval_precision': 0.6431535269709544, 'eval_recall': 0.7380952380952381, 'eval_f1': 0.6873614190687362, 'eval_accuracy': 0.9744115549215406, 'eval_runtime': 1.3939, 'eval_samples_per_second': 35.152, 'eval_steps_per_second': 2.87, 'epoch': 13.0}\n",
      " 26% 481/1850 [08:57<23:28,  1.03s/it]\n",
      "100% 4/4 [00:01<00:00,  3.62it/s]\u001b[A\n",
      "{'loss': 0.1, 'learning_rate': 1.4594594594594596e-05, 'epoch': 13.51}\n",
      " 27% 500/1850 [09:18<27:26,  1.22s/it]Saving model checkpoint to test-ner/checkpoint-500\n",
      "Configuration saved in test-ner/checkpoint-500/config.json\n",
      "Model weights saved in test-ner/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-ner/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-ner/checkpoint-500/special_tokens_map.json\n",
      " 28% 518/1850 [09:40<23:49,  1.07s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.99it/s]\u001b[A\n",
      " 75% 3/4 [00:00<00:00,  3.59it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: company seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: address seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: total seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: date seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.12554429471492767, 'eval_precision': 0.6153846153846154, 'eval_recall': 0.7619047619047619, 'eval_f1': 0.6808510638297872, 'eval_accuracy': 0.9704885877318117, 'eval_runtime': 1.3861, 'eval_samples_per_second': 35.351, 'eval_steps_per_second': 2.886, 'epoch': 14.0}\n",
      " 28% 518/1850 [09:41<23:49,  1.07s/it]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      " 30% 555/1850 [10:22<21:21,  1.01it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.01it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.09642800688743591, 'eval_precision': 0.6940639269406392, 'eval_recall': 0.7238095238095238, 'eval_f1': 0.7086247086247085, 'eval_accuracy': 0.9785128388017118, 'eval_runtime': 1.4067, 'eval_samples_per_second': 34.833, 'eval_steps_per_second': 2.843, 'epoch': 15.0}\n",
      " 30% 555/1850 [10:23<21:21,  1.01it/s]\n",
      "100% 4/4 [00:01<00:00,  3.57it/s]\u001b[A\n",
      " 32% 592/1850 [11:04<22:40,  1.08s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.98it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.12167049944400787, 'eval_precision': 0.610236220472441, 'eval_recall': 0.7380952380952381, 'eval_f1': 0.6681034482758621, 'eval_accuracy': 0.9716476462196861, 'eval_runtime': 1.3981, 'eval_samples_per_second': 35.047, 'eval_steps_per_second': 2.861, 'epoch': 16.0}\n",
      " 32% 592/1850 [11:05<22:40,  1.08s/it]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      " 34% 629/1850 [11:46<21:40,  1.07s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.99it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.11150023341178894, 'eval_precision': 0.6952789699570815, 'eval_recall': 0.7714285714285715, 'eval_f1': 0.7313769751693002, 'eval_accuracy': 0.9766405135520685, 'eval_runtime': 1.4011, 'eval_samples_per_second': 34.973, 'eval_steps_per_second': 2.855, 'epoch': 17.0}\n",
      " 34% 629/1850 [11:47<21:40,  1.07s/it]\n",
      "100% 4/4 [00:01<00:00,  3.58it/s]\u001b[A\n",
      " 36% 666/1850 [12:28<20:58,  1.06s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.94it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.12014926224946976, 'eval_precision': 0.628, 'eval_recall': 0.7476190476190476, 'eval_f1': 0.6826086956521739, 'eval_accuracy': 0.9741440798858774, 'eval_runtime': 1.4056, 'eval_samples_per_second': 34.86, 'eval_steps_per_second': 2.846, 'epoch': 18.0}\n",
      " 36% 666/1850 [12:29<20:58,  1.06s/it]\n",
      "100% 4/4 [00:01<00:00,  3.56it/s]\u001b[A\n",
      " 38% 703/1850 [13:08<19:55,  1.04s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.98it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.11715936660766602, 'eval_precision': 0.6695278969957081, 'eval_recall': 0.7428571428571429, 'eval_f1': 0.7042889390519188, 'eval_accuracy': 0.9752139800285307, 'eval_runtime': 1.3947, 'eval_samples_per_second': 35.133, 'eval_steps_per_second': 2.868, 'epoch': 19.0}\n",
      " 38% 703/1850 [13:10<19:55,  1.04s/it]\n",
      "100% 4/4 [00:01<00:00,  3.58it/s]\u001b[A\n",
      " 40% 740/1850 [13:49<18:19,  1.01it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.01it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.12456289678812027, 'eval_precision': 0.6414342629482072, 'eval_recall': 0.7666666666666667, 'eval_f1': 0.6984815618221258, 'eval_accuracy': 0.9734308131241084, 'eval_runtime': 1.403, 'eval_samples_per_second': 34.926, 'eval_steps_per_second': 2.851, 'epoch': 20.0}\n",
      " 40% 740/1850 [13:51<18:19,  1.01it/s]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      " 42% 777/1850 [14:31<16:46,  1.07it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.00it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.10972855985164642, 'eval_precision': 0.691304347826087, 'eval_recall': 0.7571428571428571, 'eval_f1': 0.7227272727272728, 'eval_accuracy': 0.978601997146933, 'eval_runtime': 1.3915, 'eval_samples_per_second': 35.213, 'eval_steps_per_second': 2.875, 'epoch': 21.0}\n",
      " 42% 777/1850 [14:32<16:46,  1.07it/s]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      " 44% 814/1850 [15:12<18:10,  1.05s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.05it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.12120088934898376, 'eval_precision': 0.6853448275862069, 'eval_recall': 0.7571428571428571, 'eval_f1': 0.7194570135746606, 'eval_accuracy': 0.9764621968616263, 'eval_runtime': 1.405, 'eval_samples_per_second': 34.876, 'eval_steps_per_second': 2.847, 'epoch': 22.0}\n",
      " 44% 814/1850 [15:13<18:10,  1.05s/it]\n",
      "100% 4/4 [00:01<00:00,  3.58it/s]\u001b[A\n",
      " 46% 851/1850 [15:54<17:17,  1.04s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.01it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.1360963135957718, 'eval_precision': 0.6461538461538462, 'eval_recall': 0.8, 'eval_f1': 0.7148936170212765, 'eval_accuracy': 0.9713801711840229, 'eval_runtime': 1.3804, 'eval_samples_per_second': 35.496, 'eval_steps_per_second': 2.898, 'epoch': 23.0}\n",
      " 46% 851/1850 [15:55<17:17,  1.04s/it]\n",
      "100% 4/4 [00:01<00:00,  3.62it/s]\u001b[A\n",
      " 48% 888/1850 [16:35<14:57,  1.07it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.00it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.1320563107728958, 'eval_precision': 0.6507936507936508, 'eval_recall': 0.780952380952381, 'eval_f1': 0.70995670995671, 'eval_accuracy': 0.9734308131241084, 'eval_runtime': 1.4159, 'eval_samples_per_second': 34.607, 'eval_steps_per_second': 2.825, 'epoch': 24.0}\n",
      " 48% 888/1850 [16:36<14:57,  1.07it/s]\n",
      "100% 4/4 [00:01<00:00,  3.55it/s]\u001b[A\n",
      " 50% 925/1850 [17:16<15:24,  1.00it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.99it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.12816014885902405, 'eval_precision': 0.6722689075630253, 'eval_recall': 0.7619047619047619, 'eval_f1': 0.7142857142857143, 'eval_accuracy': 0.9756597717546363, 'eval_runtime': 1.3913, 'eval_samples_per_second': 35.219, 'eval_steps_per_second': 2.875, 'epoch': 25.0}\n",
      " 50% 925/1850 [17:17<15:24,  1.00it/s]\n",
      "100% 4/4 [00:01<00:00,  3.60it/s]\u001b[A\n",
      " 52% 962/1850 [17:57<14:09,  1.04it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.01it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.12497729063034058, 'eval_precision': 0.6666666666666666, 'eval_recall': 0.7714285714285715, 'eval_f1': 0.7152317880794701, 'eval_accuracy': 0.9763730385164051, 'eval_runtime': 1.4098, 'eval_samples_per_second': 34.756, 'eval_steps_per_second': 2.837, 'epoch': 26.0}\n",
      " 52% 962/1850 [17:58<14:09,  1.04it/s]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      " 54% 999/1850 [18:39<13:09,  1.08it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.00it/s]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 0.15930631756782532, 'eval_precision': 0.6106870229007634, 'eval_recall': 0.7619047619047619, 'eval_f1': 0.6779661016949152, 'eval_accuracy': 0.9685271041369472, 'eval_runtime': 1.3927, 'eval_samples_per_second': 35.183, 'eval_steps_per_second': 2.872, 'epoch': 27.0}\n",
      " 54% 999/1850 [18:40<13:09,  1.08it/s]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      "{'loss': 0.0157, 'learning_rate': 9.189189189189191e-06, 'epoch': 27.03}\n",
      " 54% 1000/1850 [18:41<19:18,  1.36s/it]Saving model checkpoint to test-ner/checkpoint-1000\n",
      "Configuration saved in test-ner/checkpoint-1000/config.json\n",
      "Model weights saved in test-ner/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-ner/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-ner/checkpoint-1000/special_tokens_map.json\n",
      " 56% 1036/1850 [19:23<12:54,  1.05it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.03it/s]\u001b[A\n",
      " 75% 3/4 [00:00<00:00,  3.61it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: company seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: address seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: total seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: date seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.1382349729537964, 'eval_precision': 0.6558704453441295, 'eval_recall': 0.7714285714285715, 'eval_f1': 0.7089715536105032, 'eval_accuracy': 0.9720934379457917, 'eval_runtime': 1.4107, 'eval_samples_per_second': 34.735, 'eval_steps_per_second': 2.836, 'epoch': 28.0}\n",
      " 56% 1036/1850 [19:24<12:54,  1.05it/s]\n",
      "100% 4/4 [00:01<00:00,  3.61it/s]\u001b[A\n",
      " 58% 1073/1850 [20:05<14:01,  1.08s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.99it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.13869966566562653, 'eval_precision': 0.6991150442477876, 'eval_recall': 0.7523809523809524, 'eval_f1': 0.7247706422018347, 'eval_accuracy': 0.9755706134094151, 'eval_runtime': 1.4317, 'eval_samples_per_second': 34.225, 'eval_steps_per_second': 2.794, 'epoch': 29.0}\n",
      " 58% 1073/1850 [20:06<14:01,  1.08s/it]\n",
      "100% 4/4 [00:01<00:00,  3.58it/s]\u001b[A\n",
      " 60% 1110/1850 [20:47<12:47,  1.04s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.01it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.13958969712257385, 'eval_precision': 0.6612244897959184, 'eval_recall': 0.7714285714285715, 'eval_f1': 0.7120879120879122, 'eval_accuracy': 0.9748573466476462, 'eval_runtime': 1.4251, 'eval_samples_per_second': 34.383, 'eval_steps_per_second': 2.807, 'epoch': 30.0}\n",
      " 60% 1110/1850 [20:48<12:47,  1.04s/it]\n",
      "100% 4/4 [00:01<00:00,  3.56it/s]\u001b[A\n",
      " 62% 1147/1850 [21:28<12:03,  1.03s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.04it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.1358560025691986, 'eval_precision': 0.6482213438735178, 'eval_recall': 0.780952380952381, 'eval_f1': 0.7084233261339092, 'eval_accuracy': 0.9745007132667618, 'eval_runtime': 1.3937, 'eval_samples_per_second': 35.159, 'eval_steps_per_second': 2.87, 'epoch': 31.0}\n",
      " 62% 1147/1850 [21:29<12:03,  1.03s/it]\n",
      "100% 4/4 [00:01<00:00,  3.61it/s]\u001b[A\n",
      " 64% 1184/1850 [22:09<09:45,  1.14it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.02it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.13476650416851044, 'eval_precision': 0.690677966101695, 'eval_recall': 0.7761904761904762, 'eval_f1': 0.7309417040358746, 'eval_accuracy': 0.9762838801711841, 'eval_runtime': 1.3925, 'eval_samples_per_second': 35.189, 'eval_steps_per_second': 2.873, 'epoch': 32.0}\n",
      " 64% 1184/1850 [22:10<09:45,  1.14it/s]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      " 66% 1221/1850 [22:50<08:59,  1.17it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.03it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.1310224086046219, 'eval_precision': 0.6639344262295082, 'eval_recall': 0.7714285714285715, 'eval_f1': 0.7136563876651981, 'eval_accuracy': 0.9753031383737518, 'eval_runtime': 1.4036, 'eval_samples_per_second': 34.91, 'eval_steps_per_second': 2.85, 'epoch': 33.0}\n",
      " 66% 1221/1850 [22:52<08:59,  1.17it/s]\n",
      "100% 4/4 [00:01<00:00,  3.58it/s]\u001b[A\n",
      " 68% 1258/1850 [23:31<09:39,  1.02it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.07it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.12683513760566711, 'eval_precision': 0.711864406779661, 'eval_recall': 0.8, 'eval_f1': 0.7533632286995515, 'eval_accuracy': 0.9761055634807418, 'eval_runtime': 1.3909, 'eval_samples_per_second': 35.229, 'eval_steps_per_second': 2.876, 'epoch': 34.0}\n",
      " 68% 1258/1850 [23:33<09:39,  1.02it/s]\n",
      "100% 4/4 [00:01<00:00,  3.64it/s]\u001b[A\n",
      " 70% 1295/1850 [24:13<08:19,  1.11it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.02it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.13689091801643372, 'eval_precision': 0.7021276595744681, 'eval_recall': 0.7857142857142857, 'eval_f1': 0.7415730337078651, 'eval_accuracy': 0.9763730385164051, 'eval_runtime': 1.4134, 'eval_samples_per_second': 34.668, 'eval_steps_per_second': 2.83, 'epoch': 35.0}\n",
      " 70% 1295/1850 [24:14<08:19,  1.11it/s]\n",
      "100% 4/4 [00:01<00:00,  3.58it/s]\u001b[A\n",
      " 72% 1332/1850 [24:54<08:21,  1.03it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.03it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.13108175992965698, 'eval_precision': 0.6417322834645669, 'eval_recall': 0.7761904761904762, 'eval_f1': 0.7025862068965517, 'eval_accuracy': 0.9751248216833096, 'eval_runtime': 1.3935, 'eval_samples_per_second': 35.164, 'eval_steps_per_second': 2.871, 'epoch': 36.0}\n",
      " 72% 1332/1850 [24:56<08:21,  1.03it/s]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      " 74% 1369/1850 [25:37<07:48,  1.03it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.98it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.12597396969795227, 'eval_precision': 0.6982758620689655, 'eval_recall': 0.7714285714285715, 'eval_f1': 0.7330316742081449, 'eval_accuracy': 0.9778887303851641, 'eval_runtime': 1.3823, 'eval_samples_per_second': 35.449, 'eval_steps_per_second': 2.894, 'epoch': 37.0}\n",
      " 74% 1369/1850 [25:38<07:48,  1.03it/s]\n",
      "100% 4/4 [00:01<00:00,  3.60it/s]\u001b[A\n",
      " 76% 1406/1850 [26:19<08:01,  1.08s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.99it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.13105271756649017, 'eval_precision': 0.6735537190082644, 'eval_recall': 0.7761904761904762, 'eval_f1': 0.7212389380530974, 'eval_accuracy': 0.9755706134094151, 'eval_runtime': 1.4088, 'eval_samples_per_second': 34.78, 'eval_steps_per_second': 2.839, 'epoch': 38.0}\n",
      " 76% 1406/1850 [26:20<08:01,  1.08s/it]\n",
      "100% 4/4 [00:01<00:00,  3.60it/s]\u001b[A\n",
      " 78% 1443/1850 [27:00<06:53,  1.02s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.02it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.1310940384864807, 'eval_precision': 0.7260869565217392, 'eval_recall': 0.7952380952380952, 'eval_f1': 0.7590909090909091, 'eval_accuracy': 0.9777104136947218, 'eval_runtime': 1.403, 'eval_samples_per_second': 34.926, 'eval_steps_per_second': 2.851, 'epoch': 39.0}\n",
      " 78% 1443/1850 [27:01<06:53,  1.02s/it]\n",
      "100% 4/4 [00:01<00:00,  3.58it/s]\u001b[A\n",
      " 80% 1480/1850 [27:41<07:06,  1.15s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.03it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.14663150906562805, 'eval_precision': 0.6496062992125984, 'eval_recall': 0.7857142857142857, 'eval_f1': 0.7112068965517241, 'eval_accuracy': 0.9737874465049928, 'eval_runtime': 1.3876, 'eval_samples_per_second': 35.312, 'eval_steps_per_second': 2.883, 'epoch': 40.0}\n",
      " 80% 1480/1850 [27:43<07:06,  1.15s/it]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      "{'loss': 0.0096, 'learning_rate': 3.7837837837837844e-06, 'epoch': 40.54}\n",
      " 81% 1500/1850 [28:04<05:45,  1.01it/s]Saving model checkpoint to test-ner/checkpoint-1500\n",
      "Configuration saved in test-ner/checkpoint-1500/config.json\n",
      "Model weights saved in test-ner/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in test-ner/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in test-ner/checkpoint-1500/special_tokens_map.json\n",
      " 82% 1517/1850 [28:26<05:49,  1.05s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.98it/s]\u001b[A\n",
      " 75% 3/4 [00:00<00:00,  3.55it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: company seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: address seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: total seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: date seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.1390058994293213, 'eval_precision': 0.648, 'eval_recall': 0.7714285714285715, 'eval_f1': 0.7043478260869566, 'eval_accuracy': 0.9753922967189729, 'eval_runtime': 1.3994, 'eval_samples_per_second': 35.014, 'eval_steps_per_second': 2.858, 'epoch': 41.0}\n",
      " 82% 1517/1850 [28:27<05:49,  1.05s/it]\n",
      "100% 4/4 [00:01<00:00,  3.55it/s]\u001b[A\n",
      " 84% 1554/1850 [29:07<05:10,  1.05s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.96it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.14140687882900238, 'eval_precision': 0.6626016260162602, 'eval_recall': 0.7761904761904762, 'eval_f1': 0.7149122807017544, 'eval_accuracy': 0.9753031383737518, 'eval_runtime': 1.4067, 'eval_samples_per_second': 34.834, 'eval_steps_per_second': 2.844, 'epoch': 42.0}\n",
      " 84% 1554/1850 [29:09<05:10,  1.05s/it]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      " 86% 1591/1850 [29:48<04:25,  1.02s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.01it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.13991115987300873, 'eval_precision': 0.6612244897959184, 'eval_recall': 0.7714285714285715, 'eval_f1': 0.7120879120879122, 'eval_accuracy': 0.9761947218259629, 'eval_runtime': 1.3868, 'eval_samples_per_second': 35.332, 'eval_steps_per_second': 2.884, 'epoch': 43.0}\n",
      " 86% 1591/1850 [29:50<04:25,  1.02s/it]\n",
      "100% 4/4 [00:01<00:00,  3.61it/s]\u001b[A\n",
      " 88% 1628/1850 [30:30<03:22,  1.09it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.97it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.14984780550003052, 'eval_precision': 0.6573705179282868, 'eval_recall': 0.7857142857142857, 'eval_f1': 0.7158351409978307, 'eval_accuracy': 0.973965763195435, 'eval_runtime': 1.411, 'eval_samples_per_second': 34.727, 'eval_steps_per_second': 2.835, 'epoch': 44.0}\n",
      " 88% 1628/1850 [30:31<03:22,  1.09it/s]\n",
      "100% 4/4 [00:01<00:00,  3.57it/s]\u001b[A\n",
      " 90% 1665/1850 [31:11<03:29,  1.13s/it]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.02it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.14482463896274567, 'eval_precision': 0.6561264822134387, 'eval_recall': 0.7904761904761904, 'eval_f1': 0.7170626349892008, 'eval_accuracy': 0.9747681883024251, 'eval_runtime': 1.4067, 'eval_samples_per_second': 34.835, 'eval_steps_per_second': 2.844, 'epoch': 45.0}\n",
      " 90% 1665/1850 [31:12<03:29,  1.13s/it]\n",
      "100% 4/4 [00:01<00:00,  3.58it/s]\u001b[A\n",
      " 92% 1702/1850 [31:51<02:23,  1.03it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.02it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.14416050910949707, 'eval_precision': 0.673469387755102, 'eval_recall': 0.7857142857142857, 'eval_f1': 0.7252747252747253, 'eval_accuracy': 0.9753031383737518, 'eval_runtime': 1.4191, 'eval_samples_per_second': 34.529, 'eval_steps_per_second': 2.819, 'epoch': 46.0}\n",
      " 92% 1702/1850 [31:53<02:23,  1.03it/s]\n",
      "100% 4/4 [00:01<00:00,  3.57it/s]\u001b[A\n",
      " 94% 1739/1850 [32:32<01:39,  1.12it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.05it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.14717045426368713, 'eval_precision': 0.6587301587301587, 'eval_recall': 0.7904761904761904, 'eval_f1': 0.7186147186147186, 'eval_accuracy': 0.9740549215406562, 'eval_runtime': 1.4107, 'eval_samples_per_second': 34.734, 'eval_steps_per_second': 2.835, 'epoch': 47.0}\n",
      " 94% 1739/1850 [32:34<01:39,  1.12it/s]\n",
      "100% 4/4 [00:01<00:00,  3.62it/s]\u001b[A\n",
      " 96% 1776/1850 [33:14<01:10,  1.05it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.98it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.1458742618560791, 'eval_precision': 0.648, 'eval_recall': 0.7714285714285715, 'eval_f1': 0.7043478260869566, 'eval_accuracy': 0.974679029957204, 'eval_runtime': 1.3853, 'eval_samples_per_second': 35.372, 'eval_steps_per_second': 2.887, 'epoch': 48.0}\n",
      " 96% 1776/1850 [33:15<01:10,  1.05it/s]\n",
      "100% 4/4 [00:01<00:00,  3.56it/s]\u001b[A\n",
      " 98% 1813/1850 [33:55<00:33,  1.10it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  3.98it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.1468367725610733, 'eval_precision': 0.649402390438247, 'eval_recall': 0.7761904761904762, 'eval_f1': 0.7071583514099783, 'eval_accuracy': 0.9744115549215406, 'eval_runtime': 1.3814, 'eval_samples_per_second': 35.471, 'eval_steps_per_second': 2.896, 'epoch': 49.0}\n",
      " 98% 1813/1850 [33:56<00:33,  1.10it/s]\n",
      "100% 4/4 [00:01<00:00,  3.61it/s]\u001b[A\n",
      "100% 1850/1850 [34:37<00:00,  1.03it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:00<00:00,  4.03it/s]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 0.14775344729423523, 'eval_precision': 0.6442687747035574, 'eval_recall': 0.7761904761904762, 'eval_f1': 0.7041036717062634, 'eval_accuracy': 0.9744115549215406, 'eval_runtime': 1.4158, 'eval_samples_per_second': 34.609, 'eval_steps_per_second': 2.825, 'epoch': 50.0}\n",
      "100% 1850/1850 [34:38<00:00,  1.03it/s]\n",
      "100% 4/4 [00:01<00:00,  3.59it/s]\u001b[A\n",
      "                                 \u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2078.7701, 'train_samples_per_second': 14.143, 'train_steps_per_second': 0.89, 'train_loss': 0.035061998753934294, 'epoch': 50.0}\n",
      "100% 1850/1850 [34:38<00:00,  1.12s/it]\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 16\n",
      "100% 4/4 [00:01<00:00,  3.63it/s]\n",
      "The following columns in the test set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 82\n",
      "  Batch size = 16\n",
      " 83% 5/6 [00:01<00:00,  4.11it/s]{'ate': {'precision': 0.6068376068376068, 'recall': 0.6893203883495146, 'f1': 0.6454545454545455, 'number': 103}, 'ddress': {'precision': 0.782608695652174, 'recall': 0.8, 'f1': 0.7912087912087912, 'number': 90}, 'ompany': {'precision': 0.7088607594936709, 'recall': 0.691358024691358, 'f1': 0.7, 'number': 81}, 'otal': {'precision': 0.6605504587155964, 'recall': 0.7659574468085106, 'f1': 0.7093596059113301, 'number': 94}, 'overall_precision': 0.6826196473551638, 'overall_recall': 0.7364130434782609, 'overall_f1': 0.7084967320261438, 'overall_accuracy': 0.969463275079296}\n",
      "Saving model checkpoint to /root/.mozhi/models/hf//sroie2019v1\n",
      "Configuration saved in /root/.mozhi/models/hf//sroie2019v1/config.json\n",
      "Model weights saved in /root/.mozhi/models/hf//sroie2019v1/pytorch_model.bin\n",
      "tokenizer config file saved in /root/.mozhi/models/hf//sroie2019v1/tokenizer_config.json\n",
      "Special tokens file saved in /root/.mozhi/models/hf//sroie2019v1/special_tokens_map.json\n",
      "100% 6/6 [00:03<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "! python hf_model_train.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lHKTRylyUWBW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HFTransformerWithCustom-NER-Datset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
